{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pràctica 2 - Altres kernels\n",
    "\n",
    "En la sessió anterior hem treballat amb classificadors lineals, entre ells les SVM, en aquesta pràctica ho farem amb SVM amb _kernels_ que permeten fer transformacions a espais amb altres dimensionalitats que permeten classificadors més complexos.\n",
    "\n",
    "Cada vegada que creem una instància d'un objecte de la classe `SVC` aquest té associat un *kernel*. Cal recordar que els *kernels* són funcions matemàtiques que mapegen la nostra distribució d'entrada a una dimensió superior. Realment, només calculen la transformació entre les observacions com si estiguessin en una dimensió superior sense la necessitat de fer la transformació de manera explícita.\n",
    "\n",
    "\n",
    "## Implementant el nostre *kernel* lineal\n",
    "\n",
    "El kernel lineal és molt simple, la seva fórmula és la següent:\n",
    "\n",
    "$$K(x,z) = <x · z'>,$$\n",
    "\n",
    "A continuació veureu la seva implementació. Fixeu-vos que realitzem el producte escalar entre l'array de mostres d'entrenament `x1` i les noves observacions `x2` (transposada) de manera que obtenim una matriu on tenim una mesura de similitud entre cada una de les mostres."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def funcio_kernel_lineal(x1, x2):\n",
    "     return x1.dot(x2.T)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ara el podem emprar per classificar. El constructor de la classe  `SVC` té el paràmetre anomenat `kernel` que a més dels que ja es troben definits a la llibreria ens permet assignar funcions com la que hem definit prèviament.\n",
    "\n",
    "### Feina 1\n",
    "\n",
    "Comprovar que emprant el *kernel* definit en la funció `kernel_lineal` tenim els mateixos resultats que emprant la classe `SVC` amb el paràmetre `kernel=linear`.\n",
    "\n",
    "Per avaluar que tenim els mateixos resultats en ambdós classificadors ho farem de la següent manera:\n",
    "\n",
    "1. Entrenar (mètode `fit`) un objecte de la classe `SVC` amb el paràmetre `kernel=linear` i el conjunt d'entrenament.\n",
    "2. Entrenar un objecte de la classe `SVC` amb el paràmetre `kernel=funcio_kernel_lineal` i el conjunt d'entrenament. Així emprarà la nostra funció.\n",
    "3. Realitzar la predicció (mètode `predict`) dels dos objectes `SVC` entrenats anteriorment, ara amb el conjunt de test.\n",
    "4. Emprarem la funció `precision_score` [documentació](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) per avaluar el rendiment dels nostres classificadors. Recordau que la precisió és la relació tp / (tp + fp) on tp és el nombre de positius vertaders i fp el nombre de falsos positius. La precisió és intuïtivament la capacitat del classificador de no etiquetar com a positiva una mostra que és negativa. El millor valor és 1 i el pitjor és 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##  *kernel* gaussià\n",
    "\n",
    "\n",
    "El _kernel_ gaussià també conegut com `RBF` (_Radial Basis Function_) té la següent formulació:\n",
    "\n",
    "$$ K(x,z) = exp(-\\frac{||x-z||^2}{2\\sigma^2})$$\n",
    "\n",
    "La implementació particular de la llibreria _Scikit_ substitueix $\\frac{1}{{2\\sigma^2}}$ per $\\gamma = \\frac{1}{|caracteristiques| * \\sigma^2}$. I per tant ens queda:\n",
    "\n",
    "$$ K(x,z) = exp(-\\gamma||x-z||^2)$$\n",
    "\n",
    "Com hem definit, $\\gamma$ és la inversa de la desviació estàndard del _kernel_ `RBF` i s'utilitza com a mesura de semblança entre dos punts. Intuïtivament, un valor **gamma petit defineix una funció gaussiana amb una gran variància**. En aquest cas, dos punts es poden considerar semblants encara que estiguin lluny l'un de l'altre. D'altra banda, un valor **gamma gran significa definir una funció gaussiana amb una petita variància** i en aquest cas, dos punts es consideren semblants només si estan a prop l'un de l'altre.\n",
    " \n",
    "Consultau el _notebook_ `SVC_parameters` per veure l'efecte de modificar aquest valor en la classificació de dades.\n",
    "\n",
    "### Feina 2\n",
    "\n",
    "Implementar un _kernel_ gaussià i comprovar que obteniu els mateixos resultats que usant el _kernel_ RBF de la llibreria _Scikit_, amb els mateixos valors de `C`. La funció que implementareu ha de tenir 2 paràmetres: els conjunts de dades $x_1$ i $x_2$:\n",
    "\n",
    "```\n",
    "def kernel_gauss(x1, x2):\n",
    "     TODO: Put your code here\n",
    "```\n",
    "\n",
    "El valor de $\\gamma$, calculat amb la fórmula que ja hem comentat abans, $\\gamma = \\frac{1}{|caracteristiques| * \\sigma^2}$, serà una variable global que podrem emprar tant en aquesta tasca com en la següent. \n",
    "\n",
    "L'expressió $||x-z||^2$, norma L2, es pot definir com la distància euclidiana al quadrat entre els dos vectors de característiques. A la llibreria _Scipy_ trobareu la funció `distance_matrix` que computa la distància entre totes les parelles de valors dels dos vectors que rebi per paràmetre ([enllaç](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html)) i us ajudarà-\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## _Kernel_ polinòmic\n",
    "\n",
    "El _kernel_ lineal que hem implementat abans no és més que un cas particular del cas polinòmic que es defineix com:\n",
    "$$ K(x,z) = (\\gamma<x · z'> + r)^d $$\n",
    "\n",
    "A la llibreria _Scikit_ r es defineix amb el paràmetre `coef0` i per defecte té valor 0. El paràmetre $\\gamma$ es calcula igual que ho hem fet abans, així que podem reaprofitar el codi: \n",
    "\n",
    "$$ \\gamma = \\frac{1}{|caracteristiques| * \\sigma^2} $$\n",
    "\n",
    "### Feina 3\n",
    "\n",
    "Programar una funció que implementi el _kernel_ polinòmic i comprovar que obteniu els mateixos resultats que usant el _kernel_ `poly` de la llibreria _Scikit_, amb els mateixos valors de `C` i `degree`. La funció que implementareu ha de tenir 3 paràmetres: els conjunts de dades $x_1$ i $x_2$ i el grau del polinomi.\n",
    "\n",
    "```\n",
    "def kernel_poly(x1, x2, degree=3):\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dibuixant les fronteres de decisió\n",
    "\n",
    "Veure que som capaços de reproduir els mateixos que _Scikit_ està molt bé, però l'objectiu d'aquest exercici també és entendre com canvien les fronteres de decisió en funció dels diferents _kernels_. Fer això és relativament senzill emprant la funció `DecisionBoundaryDisplay.from_estimator`.\n",
    "\n",
    "A la documentació en podreu veure un exemple complet i funcional [enllaç](https://scikit-learn.org/dev/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html).\n",
    "\n",
    "### Feina 4\n",
    "\n",
    "Dibuixar les fronteres de decisió dels 3 classificadors que hem creat amb els nostres _kernels_.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
